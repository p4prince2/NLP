{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c188fa7c",
   "metadata": {},
   "source": [
    "# Roadmap to Learn NLP\n",
    "\n",
    "This notebook provides a structured roadmap for learning Natural Language Processing (NLP), \n",
    "starting from basic text processing techniques to advanced deep learning models like Transformers (BERT, GPT).\n",
    "\n",
    "We will also visualize the roadmap as a **pyramid structure** using Python's Matplotlib and NetworkX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa94d62",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Cleaning Input (Preprocessing)\n",
    "- **Techniques**: Tokenization, Lemmatization, Stemming\n",
    "- **Purpose**: Converts raw text into a cleaner, more usable format.\n",
    "\n",
    "## 2. Converting Input Text to Vector Representations\n",
    "- **BoW (Bag of Words)**: Counts word occurrences.\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Weighs words based on importance.\n",
    "- **Unigram Models**: Uses single-word sequences.\n",
    "\n",
    "## 3. Advanced Text Vectorization\n",
    "- **Word2Vec**: Converts words into dense vectors based on context.\n",
    "- **GloVe**: Captures word meaning based on co-occurrence.\n",
    "- **FastText**: Similar to Word2Vec but considers subword information.\n",
    "\n",
    "## 4. Recurrent Neural Networks (RNNs)\n",
    "- **LSTM (Long Short-Term Memory)**: Handles long-term dependencies.\n",
    "- **GRU (Gated Recurrent Units)**: A simplified LSTM with fewer parameters.\n",
    "\n",
    "## 5. Word Embeddings & Contextual Representations\n",
    "- **Pre-trained models like Word2Vec, GloVe, FastText**.\n",
    "\n",
    "## 6. Transformer Models\n",
    "- **Attention-based models replacing RNNs**.\n",
    "- **More efficient for parallel computation**.\n",
    "\n",
    "## 7. Advanced Deep Learning Models for NLP\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "- **GPT (Generative Pre-trained Transformer)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4804909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Define pyramid structure with explicit hierarchy levels\n",
    "stages = [\n",
    "    (\"Cleaning Input\", \"Input Text to Vector (BoW, TF-IDF, Unigram)\"),\n",
    "    (\"Input Text to Vector (BoW, TF-IDF, Unigram)\", \"Input Text to Vector (Word2Vec, GloVe)\"),\n",
    "    (\"Input Text to Vector (Word2Vec, GloVe)\", \"RNN, LSTM, GRU\"),\n",
    "    (\"RNN, LSTM, GRU\", \"Word Embeddings\"),\n",
    "    (\"Word Embeddings\", \"Transformer\"),\n",
    "    (\"Transformer\", \"BERT, GPT\")\n",
    "]\n",
    "\n",
    "# Create graph\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(stages)\n",
    "\n",
    "# Define node levels for proper layout\n",
    "node_levels = {\n",
    "    \"Cleaning Input\": 0,\n",
    "    \"Input Text to Vector (BoW, TF-IDF, Unigram)\": 1,\n",
    "    \"Input Text to Vector (Word2Vec, GloVe)\": 2,\n",
    "    \"RNN, LSTM, GRU\": 3,\n",
    "    \"Word Embeddings\": 4,\n",
    "    \"Transformer\": 5,\n",
    "    \"BERT, GPT\": 6\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "pos = {node: (0, level) for node, level in node_levels.items()}  # Vertical pyramid layout\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=3000, font_size=10, font_weight='bold')\n",
    "plt.title(\"NLP Learning Roadmap\", fontsize=14)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
