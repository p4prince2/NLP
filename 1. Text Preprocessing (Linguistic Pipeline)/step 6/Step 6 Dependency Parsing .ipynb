{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "630ec164-336a-4dcb-b28a-508512862cd3",
   "metadata": {},
   "source": [
    "## Step 6: Dependency Parsing\n",
    "\n",
    "Dependency Parsing is used to find that how all the words in the sentence are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08275159-8790-44f2-ae8f-c5b641370850",
   "metadata": {},
   "source": [
    "# Dependency Parsing in NLP\n",
    "\n",
    "**Dependency parsing** is a natural language processing (NLP) technique used to analyze the grammatical structure of a sentence by establishing relationships between words. It helps in understanding how words are connected, which is useful for tasks like **information extraction, question answering, and machine translation**.\n",
    "\n",
    "## **Key Concepts in Dependency Parsing**\n",
    "\n",
    "- **Head and Dependent** – Every word in a sentence (except the root) depends on another word called its **head**.\n",
    "- **Dependency Relations** – The type of relationship between the head and its dependent (e.g., subject, object, modifier).\n",
    "- **Root** – The main verb of the sentence, which serves as the central node in the dependency tree.\n",
    "\n",
    "## **Example**\n",
    "\n",
    "### **Sentence:**  \n",
    "**\"The cat sat on the mat.\"**  \n",
    "\n",
    "### **Dependency Parse:**  \n",
    "- **sat** (ROOT)  \n",
    "- **cat** → subject of \"sat\" (**nsubj**)  \n",
    "- **on** → preposition modifying \"sat\" (**prep**)  \n",
    "  - **mat** → object of \"on\" (**pobj**)  \n",
    "- **The** → determiner modifying \"cat\" (**det**)  \n",
    "- **The** → determiner modifying \"mat\" (**det**)  \n",
    "\n",
    "## **Implementation in Python (Using spaCy)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9541a1-6d1f-40f2-8b96-a317fb68c944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.5-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.1-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\p4pri\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.5-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.8 MB 2.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/11.8 MB 2.1 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.8/11.8 MB 2.5 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.1/11.8 MB 2.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.9/11.8 MB 2.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.4/11.8 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.9/11.8 MB 2.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.5/11.8 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.0/11.8 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.5/11.8 MB 2.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.3/11.8 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.8/11.8 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/11.8 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.9/11.8 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.7/11.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.2/11.8 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/11.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 524.3/632.6 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 632.6/632.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.1-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.3 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.4/6.3 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.3 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.4/6.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.9/6.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.4 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.8/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.1/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.6/5.4 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 3.7/5.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.1 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 shellingham-1.5.4 spacy-3.8.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\p4pri\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\p4pri\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\p4pri\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2762d7e7-f35c-4c9b-b4ff-3048c34f0568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The --> det --> cat\n",
      "cat --> nsubj --> sat\n",
      "sat --> ROOT --> sat\n",
      "on --> prep --> sat\n",
      "the --> det --> mat\n",
      "mat --> pobj --> on\n",
      ". --> punct --> sat\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print dependency relations\n",
    "for token in doc:\n",
    "    print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe1144-79a6-423c-969d-4ae4e51e588d",
   "metadata": {},
   "source": [
    "### **Expected Output:**  \n",
    "```\n",
    "The --> det --> cat\n",
    "cat --> nsubj --> sat\n",
    "sat --> ROOT --> sat\n",
    "on --> prep --> sat\n",
    "the --> det --> mat\n",
    "mat --> pobj --> on\n",
    ". --> punct --> sat\n",
    "```\n",
    "This output shows how each word in the sentence is related to another in the dependency tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87885117-492c-4a32-ae44-1907a24e1944",
   "metadata": {},
   "source": [
    "# spaCy vs. NLTK - A Comparison\n",
    "\n",
    "Both **spaCy** and **NLTK** are popular **Natural Language Processing (NLP) libraries**, but they serve different purposes. This notebook provides a detailed comparison.\n",
    "\n",
    "## **1. Overview**\n",
    "| Feature  | spaCy  | NLTK  |\n",
    "|----------|--------|-------|\n",
    "| **Purpose** | Industrial-grade NLP | Research & educational NLP |\n",
    "| **Speed** | Faster (optimized in Cython) | Slower (pure Python implementation) |\n",
    "| **Ease of Use** | Simple, efficient API | More modular but complex |\n",
    "| **Pre-trained Models** | Yes (e.g., `en_core_web_sm`) | No (requires external models) |\n",
    "| **Deep Learning Integration** | Supports deep learning (via `spacy-transformers`) | Limited support |\n",
    "| **Use Case** | Production-ready applications | Academic research & prototyping |\n",
    "\n",
    "## **2. Feature Comparison**\n",
    "| Feature | spaCy | NLTK |\n",
    "|---------|--------|------|\n",
    "| **Tokenization** | Rule-based, fast, supports custom models | Rule-based, flexible but slower |\n",
    "| **POS Tagging** | Uses statistical models (accurate) | Uses rule-based and statistical models |\n",
    "| **Dependency Parsing** | Yes (built-in, efficient) | Requires Stanford Parser (external) |\n",
    "| **Named Entity Recognition (NER)** | Yes (pre-trained models available) | No built-in NER (requires external models) |\n",
    "| **Lemmatization** | Fast, model-based | WordNet-based (slower) |\n",
    "| **Stemming** | Not available (uses lemmatization instead) | Yes (Porter, Snowball, Lancaster) |\n",
    "| **Word Embeddings** | Supports `Word2Vec`, `GloVe`, `fastText`, `transformers` | No direct support |\n",
    "| **Sentiment Analysis** | Requires external models | Uses `VADER`, `TextBlob` |\n",
    "| **Text Classification** | Supports custom models via `spacy.pipeline` | Uses `nltk.classify` (manual implementation) |\n",
    "| **Stopwords** | Inbuilt stopword list | Has stopword lists for multiple languages |\n",
    "| **Multi-language Support** | Yes, supports multiple languages | Yes, but relies on external corpora |\n",
    "\n",
    "## **3. Performance & Speed**\n",
    "- **spaCy** is significantly **faster** than NLTK.\n",
    "- **NLTK** is slower due to its modular nature.\n",
    "\n",
    "## **4. Code Comparison**\n",
    "### **Tokenization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b96b63c-a020-4463-aaba-74291b4276c7",
   "metadata": {},
   "source": [
    "***spaCy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6f00aa1-82eb-4fb5-8bb4-394ce1cf847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2019fb-b561-4cdd-b894-89384834ad6c",
   "metadata": {},
   "source": [
    "\n",
    "***NLTK***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a8cf057-0f57-4ab7-9715-2e626529b420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4fa64f-f4e9-4800-bdf6-400f4037e447",
   "metadata": {},
   "source": [
    "## **5. When to Use What?**\n",
    "| Use Case | Use **spaCy** | Use **NLTK** |\n",
    "|----------|--------------|--------------|\n",
    "| **Fast NLP processing (production apps)** | ✅ | ❌ |\n",
    "| **Machine learning & deep learning integration** | ✅ | ❌ |\n",
    "| **Tokenization, POS tagging, dependency parsing** | ✅ | ✅ (but slower) |\n",
    "| **Named Entity Recognition (NER)** | ✅ | ❌ |\n",
    "| **Educational, research, academic work** | ❌ | ✅ |\n",
    "| **Corpus-based NLP experiments** | ❌ | ✅ |\n",
    "| **Text classification with custom models** | ✅ | ✅ |\n",
    "\n",
    "## **6. Conclusion**\n",
    "- **Use spaCy if you need** a **fast, production-ready, and efficient NLP library**.\n",
    "- **Use NLTK if you need** more **customization, linguistic resources, and corpus-based experiments**.\n",
    "\n",
    "### **💡 Recommendation**\n",
    "- If you are building **real-world NLP applications** → **Use spaCy** 🚀  \n",
    "- If you are doing **NLP research and learning NLP fundamentals** → **Use NLTK** 📚  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e2931-e937-4370-8125-8c6d2d8be6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
